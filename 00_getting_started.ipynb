{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to PyTorch\n",
    "\n",
    "Tutorial link(s):\n",
    "* https://www.youtube.com/watch?v=c36lUUr864M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6889, 0.2939, 0.4128])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.4932e-36, 1.4013e-45, 3.4929e-36],\n",
       "        [1.4013e-45, 9.0067e-35, 1.4013e-45]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(2,3) \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2705, 0.1732, 0.7292],\n",
       "        [0.1554, 0.4457, 0.9065]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(2,3)\n",
    "x.add_(y)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reshaped = y.view(6)\n",
    "y_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27049989, 0.17324698, 0.72922385],\n",
       "       [0.15536177, 0.44574386, 0.9065053 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.numpy()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2705   , 1.173247 , 1.7292238],\n",
       "       [1.1553618, 1.4457438, 1.9065053]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z += 1\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2705, 1.1732, 1.7292],\n",
       "        [1.1554, 1.4457, 1.9065]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z and x share memory space because they both reside in the CPU\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[10,9],[8,7]])\n",
    "b = torch.tensor([[1,0],[0,-1]])\n",
    "torch.mm(a,b)\n",
    "torch.dot(a[0], b[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0526,  0.6679, -0.0847], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad = torch.randn(3, requires_grad=True)\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2119, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating backprop function MeanBackward0\n",
    "z_grad = x_grad + 2 \n",
    "z_grad = z_grad.mean() \n",
    "z_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x_grad.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2119, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector Jacobian product\n",
    "# no argument: scalar\n",
    "# tensor argument needed otherwise\n",
    "z_grad.backward() \n",
    "z_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.3333, 0.3333])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0526,  0.6679, -0.0847])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_grad.requires_grad_(False)\n",
    "# x_grad.detach()\n",
    "# with torch.no_grad()\n",
    "y_grad = x_grad.detach()\n",
    "y_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 0.6667, 0.6667])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient accumulates\n",
    "z_grad.backward() \n",
    "x_grad.grad\n",
    "#x_grad.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass\n",
    "y_hat = w * x\n",
    "\n",
    "# compute loss\n",
    "l = (y_hat - y)**2\n",
    "print(l)\n",
    "\n",
    "# backward pass\n",
    "l.backward()\n",
    "print(w.grad)\n",
    "\n",
    "# update weights\n",
    "learning_rate = 0.01\n",
    "new_w = w - learning_rate * w.grad\n",
    "\n",
    "# don't forget to zero the gradients\n",
    "# w.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual vs. Autograd vs. PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[M] Prediction before training: f(5) = 0.000\n",
      "[T] Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 1: w_torch = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 11: w_torch = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 2.000, loss = 0.00000000\n",
      "epoch 21: w_torch = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 2.000, loss = 0.00000000\n",
      "epoch 31: w_torch = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 2.000, loss = 0.00000000\n",
      "epoch 41: w_torch = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 2.000, loss = 0.00000000\n",
      "epoch 51: w_torch = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000000\n",
      "epoch 61: w_torch = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 71: w_torch = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w_torch = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w_torch = 2.000, loss = 0.00000000\n",
      "[M] Prediction after training: f(5) = 10.000\n",
      "[T] Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "\n",
    "# here : f = 2 * x\n",
    "x = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "x_auto = torch.tensor(x, dtype=torch.float32)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "y_auto = torch.tensor(y, dtype=torch.float32)\n",
    "y_torch = torch.tensor(y, dtype=torch.float32)\n",
    "w = 0.0\n",
    "w_auto = torch.tensor(w, dtype=torch.float32, requires_grad=True)\n",
    "w_torch = torch.tensor(w, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model output\n",
    "def forward(w, x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x (w*x - y)\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'[M] Prediction before training: f(5) = {forward(w,5):.3f}')\n",
    "# print(f'[A] Prediction before training: f(5) = {forward(w_auto,5):.3f}')\n",
    "print(f'[T] Prediction before training: f(5) = {forward(w_torch,5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss_torch = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w_torch], lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_hat = forward(w, x)\n",
    "    # y_hat_auto = forward(w_auto, x_auto)\n",
    "    y_hat_torch = forward(w_torch, x_torch)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y, y_hat)\n",
    "    # l_auto = loss(y_auto, y_hat_auto)\n",
    "    l_torch = loss_torch(y_torch, y_hat_torch)\n",
    "\n",
    "    # calculate gradients\n",
    "    dw = gradient(x, y, y_hat)\n",
    "    # l_auto.backward()\n",
    "    l_torch.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "    # with torch.no_grad():\n",
    "    #     w_auto -= learning_rate * w_auto.grad\n",
    "    # w_auto.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        # print(f'epoch {epoch+1}: w_auto = {w_auto:.3f}, loss = {l_auto:.8f}')\n",
    "        print(f'epoch {epoch+1}: w_torch = {w_torch:.3f}, loss = {l_torch:.8f}')\n",
    "\n",
    "print(f'[M] Prediction after training: f(5) = {forward(w, 5):.3f}')\n",
    "# print(f'[A] Prediction after training: f(5) = {forward(w_auto, 5):.3f}')\n",
    "print(f'[T] Prediction after training: f(5) = {forward(w_torch, 5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss = 5862.85937500\n",
      "epoch 11: loss = 4339.53515625\n",
      "epoch 21: loss = 3238.61523438\n",
      "epoch 31: loss = 2442.05541992\n",
      "epoch 41: loss = 1865.09558105\n",
      "epoch 51: loss = 1446.78247070\n",
      "epoch 61: loss = 1143.21533203\n",
      "epoch 71: loss = 922.73223877\n",
      "epoch 81: loss = 762.46923828\n",
      "epoch 91: loss = 645.89489746\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# prepare data\n",
    "X_np, y_np = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32)\n",
    "y = y.view(y.shape[0],1) # 100 -> 100,1\n",
    "\n",
    "# build model\n",
    "n_samples, n_features = X.shape\n",
    "model = nn.Linear(n_features, 1)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, y)\n",
    "\n",
    "    # calculate gradients (backward pass)\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights (and zero gradients out for next iteration)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: loss = {loss:.8f}')\n",
    "\n",
    "# plot\n",
    "y_predicted = model(X).detach().numpy()\n",
    "\n",
    "plt.plot(X_np, y_np, 'ro')\n",
    "plt.plot(X_np, y_predicted, 'b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare the data\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target\n",
    "n_samples, n_features = X.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_orig = X_train.copy()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# convert the data to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# reshape our labels\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# peek into the data distribution for one of the features\n",
    "# plt.hist(X_train_orig[:,1], bins=25)\n",
    "plt.hist(X_train_orig[:,1], bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss = 1.01718771\n",
      "epoch 11: loss = 0.73686498\n",
      "epoch 21: loss = 0.58180779\n",
      "epoch 31: loss = 0.49036747\n",
      "epoch 41: loss = 0.43044150\n",
      "epoch 51: loss = 0.38796011\n",
      "epoch 61: loss = 0.35609296\n",
      "epoch 71: loss = 0.33116075\n",
      "epoch 81: loss = 0.31101552\n",
      "epoch 91: loss = 0.29432115\n",
      "tensor(0.9561)\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y_predicted = torch.sigmoid(self.linear(X))\n",
    "        return y_predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "# model = nn.Linear(n_features, 1)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_predicted = model(X_train)\n",
    "    # y_predicted = torch.sigmoid(y_predicted) # activation function for logistic regression\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "\n",
    "    # calculate gradients (backward pass)\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights (and zero gradients out for next iteration)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: loss = {loss:.8f}')\n",
    "\n",
    "# plot\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / (y_test.shape[0])\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        from sklearn.datasets import load_wine        \n",
    "        dataset = load_wine()\n",
    "        self.x = torch.from_numpy(dataset['data'])\n",
    "        self.y = torch.from_numpy(dataset['target'])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_wine()\n",
    "# x = torch.from_numpy(dataset['data'])\n",
    "# y = torch.from_numpy(dataset['target'])\n",
    "x = dataset['data']\n",
    "y = dataset['target']\n",
    "n_samples = y.shape[0]\n",
    "type(x), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3500e+01, 3.1200e+00, 2.6200e+00, 2.4000e+01, 1.2300e+02, 1.4000e+00,\n",
      "         1.5700e+00, 2.2000e-01, 1.2500e+00, 8.6000e+00, 5.9000e-01, 1.3000e+00,\n",
      "         5.0000e+02],\n",
      "        [1.1760e+01, 2.6800e+00, 2.9200e+00, 2.0000e+01, 1.0300e+02, 1.7500e+00,\n",
      "         2.0300e+00, 6.0000e-01, 1.0500e+00, 3.8000e+00, 1.2300e+00, 2.5000e+00,\n",
      "         6.0700e+02],\n",
      "        [1.2360e+01, 3.8300e+00, 2.3800e+00, 2.1000e+01, 8.8000e+01, 2.3000e+00,\n",
      "         9.2000e-01, 5.0000e-01, 1.0400e+00, 7.6500e+00, 5.6000e-01, 1.5800e+00,\n",
      "         5.2000e+02],\n",
      "        [1.2420e+01, 2.5500e+00, 2.2700e+00, 2.2000e+01, 9.0000e+01, 1.6800e+00,\n",
      "         1.8400e+00, 6.6000e-01, 1.4200e+00, 2.7000e+00, 8.6000e-01, 3.3000e+00,\n",
      "         3.1500e+02]], dtype=torch.float64) tensor([2, 1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset = WineDataset()\n",
    "# increasing the num_workers causes an error presumably related to running this in Jupyter\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "dataiter = iter(dataloader)\n",
    "data = dataiter.next()\n",
    "features, labels = data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n",
      "epoch 1/2, step 5/45, torch.Size([4, 13])\n",
      "epoch 1/2, step 10/45, torch.Size([4, 13])\n",
      "epoch 1/2, step 15/45, torch.Size([4, 13])\n",
      "epoch 1/2, step 20/45, torch.Size([4, 13])\n",
      "epoch 1/2, step 25/45, torch.Size([4, 13])\n",
      "epoch 1/2, step 30/45, torch.Size([4, 13])\n",
      "epoch 1/2, step 35/45, torch.Size([4, 13])\n",
      "epoch 1/2, step 40/45, torch.Size([4, 13])\n",
      "epoch 1/2, step 45/45, torch.Size([2, 13])\n",
      "epoch 2/2, step 5/45, torch.Size([4, 13])\n",
      "epoch 2/2, step 10/45, torch.Size([4, 13])\n",
      "epoch 2/2, step 15/45, torch.Size([4, 13])\n",
      "epoch 2/2, step 20/45, torch.Size([4, 13])\n",
      "epoch 2/2, step 25/45, torch.Size([4, 13])\n",
      "epoch 2/2, step 30/45, torch.Size([4, 13])\n",
      "epoch 2/2, step 35/45, torch.Size([4, 13])\n",
      "epoch 2/2, step 40/45, torch.Size([4, 13])\n",
      "epoch 2/2, step 45/45, torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        # forward, backward, update\n",
    "        if not (i+1) % 5:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, {inputs.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        # data loading\n",
    "        from sklearn.datasets import load_wine        \n",
    "        dataset = load_wine()\n",
    "        self.x = dataset['data']\n",
    "        self.y = dataset['target']\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        sample = self.x[index], np.ndarray(self.y[index])\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "class ToTensor():\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "\n",
    "class MulTransform():\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      "tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n",
      "        1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n",
      "        4.2600e+03], dtype=torch.float64)\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# dataset = WineDataset(transform=ToTensor())\n",
    "dataset = WineDataset(transform=None)\n",
    "data = dataset[0]\n",
    "features, labels = data\n",
    "print(features)\n",
    "\n",
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(4)])\n",
    "dataset = WineDataset(transform=composed)\n",
    "\n",
    "data = dataset[0]\n",
    "features, labels = data\n",
    "print(features)\n",
    "print(type(features), type(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss (good): 0.3567\n",
      "loss (bad): 2.3026\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(predicted, actual):\n",
    "    # no need to np.sum OR / float(predicted.shape[0])\n",
    "    # due to one-hot encoding of actual\n",
    "    loss = np.dot( -actual, np.log(predicted) )\n",
    "    return loss\n",
    "\n",
    "y = np.array([1,0,0])\n",
    "# example good/bad output of softmax\n",
    "y_pred_good = np.array([0.7,0.2,0.1])\n",
    "y_pred_bad = np.array([0.1,0.3,0.6])\n",
    "\n",
    "l1 = cross_entropy(y_pred_good, y)\n",
    "l2 = cross_entropy(y_pred_bad, y)\n",
    "\n",
    "print(f'loss (good): {l1:.4f}')\n",
    "print(f'loss (bad): {l2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "# Multi-class\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# For 2 classes use BCELoss + sigmoid at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss (good): 0.4170\n",
      "loss (bad): 2.2200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([2.5000]), tensor([1]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not one-hot encoded prediction\n",
    "y = torch.tensor([0])\n",
    "\n",
    "# example good/bad output. no softmax\n",
    "# n_samples x n_classes -> 1x3\n",
    "y_pred_good = torch.tensor([ [2.0,1.0,0.1] ])\n",
    "y_pred_bad = torch.tensor([ [0.5,2.5,0.3] ])\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "l1 = loss(y_pred_good, y)\n",
    "l2 = loss(y_pred_bad, y)\n",
    "\n",
    "print(f'loss (good): {l1.item():.4f}')\n",
    "print(f'loss (bad): {l2.item():.4f}')\n",
    "\n",
    "pred_val1, pred_idx1 = torch.max(y_pred_bad, 1)\n",
    "pred_val1, pred_idx1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
